{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part0 Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import psycopg2\n",
    "import sqlalchemy as db\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "import contextily as ctx\n",
    "from geoalchemy2 import Geometry\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import unittest\n",
    "import numpy as np\n",
    "from unittest.mock import patch\n",
    "# warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Downloading 311 and Tree data from API\n",
    "- Download 311 and Tree data using API and Python Code\n",
    "- 311 Data set is pretty huge, so we choose to download it seperately into subfiles by year, and merge it at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Function (1) Manually Doanload Data from NYC Open Data\n",
    "def download_data(url, app_token, filename, date_field, start_date, end_date, date_format=\"%Y-%m-%dT%H:%M:%S\", limit=10000):\n",
    "    \"\"\"\n",
    "    Downloads data from the specified NYC Open Data URL within a given date range.\n",
    "\n",
    "    Args:\n",
    "        url (str): The API endpoint for the dataset.\n",
    "        app_token (str): Application token for authenticated access.\n",
    "        filename (str): The name of the file where the data will be saved.\n",
    "        date_field (str): The name of the date field in the dataset.\n",
    "        start_date (datetime): The start date for filtering data.\n",
    "        end_date (datetime): The end date for filtering data.\n",
    "        date_format (str): Format of the date fields, defaults to '%Y-%m-%dT%H:%M:%S'.\n",
    "        limit (int): Number of records to retrieve per request, defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "        None: This function writes the downloaded data to a file and does not return anything.\n",
    "    \"\"\"\n",
    "    \n",
    "    offset = 0\n",
    "    start_date_str = start_date.strftime(date_format) # Format the start date\n",
    "    end_date_str = end_date.strftime(date_format) # Format the end date\n",
    "    # Construct the query for filtering data by date range\n",
    "    date_query = f\"$where={date_field} between '{start_date_str}' and '{end_date_str}'\"\n",
    "    \n",
    "    first_batch = True  # Flag to identify the first batch of data\n",
    "    while True:\n",
    "        # Construct the full URL with necessary query parameters\n",
    "        full_url = f\"{url}?$$app_token={app_token}&{date_query}&$limit={limit}&$offset={offset}\"\n",
    "        response = requests.get(full_url) # Perform the API request\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.text\n",
    "            records_retrieved = data.count('\\n')  # Count the number of lines (records) retrieved\n",
    "\n",
    "            if first_batch and records_retrieved > 0:  # If this is the first batch and it contains data\n",
    "                with open(filename, 'w') as file:\n",
    "                    file.write(data) # Write data to file, including header\n",
    "                first_batch = False\n",
    "            elif records_retrieved > 1:  # For subsequent batches, skip the header row\n",
    "                with open(filename, 'a') as file:\n",
    "                    file.write(data.split('\\n', 1)[1])  # Append data to file without header\n",
    "\n",
    "            if records_retrieved < limit + 1:   # Check if all records have been retrieved\n",
    "                break\n",
    "            offset += limit # Increment the offset for the next batch\n",
    "        else:\n",
    "            print(f\"Failed to download data at offset {offset}: Status code {response.status_code}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#app Toekn: Application token used for authentication\n",
    "app_token = 'Z8lDMDpdnonlT1RjM5YGII6Ii'\n",
    "#Data URL: Defines the online API URLs for the datasets\n",
    "url_311 = 'https://data.cityofnewyork.us/resource/erm2-nwe9.csv'\n",
    "url_trees = 'https://data.cityofnewyork.us/resource/5rq2-4hqu.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download tree data: Initiates the download of tree data\n",
    "# date format \"%m/%d/%Y\": Setting the date format to month/day/year\n",
    "download_data(\n",
    "    url=url_trees,\n",
    "    app_token=app_token,  \n",
    "    filename=\"data/tree_data.csv\",\n",
    "    date_field=\"created_at\",  \n",
    "    start_date=datetime(2015, 1, 1),\n",
    "    end_date=datetime(2015, 12, 31),\n",
    "    date_format=\"%m/%d/%Y\",  \n",
    "    limit=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download 311 data from 2015.1.1-2023.9.30\n",
    "#create a new folder to save 311 data by year\n",
    "subfolder_name = \"311_data\"\n",
    "subfolder_path = os.path.join(\"data\", subfolder_name)\n",
    "if not os.path.exists(subfolder_path):\n",
    "    os.makedirs(subfolder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run download data function by year\n",
    "#Year 2015\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2015.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2015, 1, 1, 0, 0),  # This represents 2015-01-01 00:00:00 AM\n",
    "    end_date=datetime(2015, 12, 31, 23, 59, 59),  # This represents 2023-09-30 11:59:59 PM\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Test (1)\n",
    "# Path to the file you expect to exist\n",
    "filename = \"data/311_data/311_data_2015.csv\"\n",
    "# Assert that the file exists\n",
    "assert os.path.exists(filename), \"File does not exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2016\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2016.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2016, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2016, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2017\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2017.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2017, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2017, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2018\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2018.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2018, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2018, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2019\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2019.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2019, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2019, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2020\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2020.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2020, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2020, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2021\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2021.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2021, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2021, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2022\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2022.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2022, 1, 1, 0, 0), \n",
    "    end_date=datetime(2022, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2023\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2023.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2023, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2023, 9, 30, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cleaning & Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Data Description & Columns to keep\n",
    "\n",
    "Prior to developing data cleaning functions, we conducted a thorough examination of each dataset's types and descriptions, as well as the query and visualization requirements of our project. The following list details the columns we have chosen to retain. Each column is accompanied by its description and original data type.\n",
    "\n",
    "##### *Zillow Rent Data Description\n",
    "\n",
    "| Column Name                  | Description                                                                          | Type        |\n",
    "|------------------------------|--------------------------------------------------------------------------------------|-------------|\n",
    "| RegionID                     | Used for pandas, an identifier for the region                                         | Integer     |\n",
    "| RegionName                   | Same as postcode, matches 'Incident Zip' in other datasets, link with latitude and longitude | Integer     |\n",
    "| City                         | Different cities, for later filtering to New York                                     | Object      |\n",
    "| Average Housing Price Columns| Keep all columns related to average housing prices for each region                   | Float64     |\n",
    "\n",
    "\n",
    "##### *311 Data\n",
    "https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9\n",
    "\n",
    "311 Service Requests from 2010 to Present\n",
    "This dataset comprises all 311 Service Requests from 2010 to the present day, updated daily. It provides a comprehensive overview of non-emergency requests and complaints to New York City's 311 service.\n",
    "\n",
    "\n",
    "| Column Name    | Description                                                              | Type        |\n",
    "|----------------|--------------------------------------------------------------------------|-------------|\n",
    "| unique_key     | Unique identifier of a Service Request (SR) in the open data set         | Plain Text  |\n",
    "| created_date   | Date SR was created                                                      | Date & Time |\n",
    "| complaint_type | First level of a hierarchy identifying the topic of the incident or condition | Plain Text  |\n",
    "| incident_zip   | Incident location zip code, provided by geo validation                   | Plain Text  |\n",
    "| latitude       | Geo based Latitude of the incident location                              | Number      |\n",
    "| longitude      | Geo based Longitude of the incident location                             | Number      |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### *Tree Data Description\n",
    "https://data.cityofnewyork.us/Environment/2015-Street-Tree-Census-Tree-Data/uvpi-gqnh\n",
    "\n",
    "\n",
    "2015 Street Tree Census, conducted by volunteers and staff organized by NYC Parks & Recreation and partner organizations. Tree data collected includes tree species, diameter and perception of health. Accompanying blockface data is available indicating status of data collection and data release citywide.\n",
    "\n",
    "\n",
    "| Column Name   | Description                                                                   | Type       |\n",
    "|---------------|-------------------------------------------------------------------------------|------------|\n",
    "| tree_id       | Unique identification number for each tree point                              | Integer     |\n",
    "| status        | Indicates whether the tree is alive, standing dead, or a stump               | Plain Text |\n",
    "| created_at  | Date and time when the tree data was created                                  | Plain Text |\n",
    "| zipcode       | Five-digit zipcode in which tree is located                                  | Integer |\n",
    "| latitude      | Latitude of point, in decimal degrees                                        | Number     |\n",
    "| longitude     | Longitude of point, in decimal degrees                                       | Number     |\n",
    "| health        | Indicates the user's perception of tree health                               | Plain Text |\n",
    "| spc_common    | Common name for species, e.g., \"red maple\"                                   | Plain Text |\n",
    "\n",
    "\n",
    "#### *Zipcode Data Description\n",
    "| Column Name | Description                                | Type    |\n",
    "|-------------|--------------------------------------------|---------|\n",
    "| ZIPCODE     | The postal code corresponding to the area  | Plain Text |\n",
    "| geometry    | Geometrical data representing the area     | Geometry |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2.1 Functions aiding data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function (2) create a remove_column function for deleting the unnecessary columns\n",
    "\n",
    "def remove_column(df, keep_columns, include_date_columns=False, date_pattern=r'\\d{4}-\\d{2}-\\d{2}'):\n",
    "    \"\"\"\n",
    "    Removes columns from a DataFrame, retaining only the specified columns and optionally any date columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to be modified.\n",
    "    - keep_columns (list of str): A list of column names to retain in the DataFrame.\n",
    "    - include_date_columns (bool, optional): Flag to include columns with date format. Defaults to False.\n",
    "    - date_pattern (str, optional): Regular expression pattern to identify date columns. Used when 'include_date_columns' is True. Default pattern matches 'YYYY-MM-DD'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The modified DataFrame with only the specified columns retained.\n",
    "    \n",
    "    Example:\n",
    "    >>> df = pd.DataFrame(...)\n",
    "    >>> new_df = remove_column(df, ['column1', 'column2'], include_date_columns=True)\n",
    "    \"\"\"\n",
    "    # Combine specified columns with date columns if needed\n",
    "    all_columns_to_keep = keep_columns\n",
    "\n",
    "    # If including date columns, append them to the list of columns to keep\n",
    "    if include_date_columns:\n",
    "        # Identify date columns using the regex pattern\n",
    "        date_columns = df.columns[df.columns.str.contains(date_pattern)]\n",
    "        all_columns_to_keep += date_columns.tolist()\n",
    "\n",
    "    # Return the DataFrame with only the specified columns retained\n",
    "    return df.loc[:, all_columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test (2)\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'column1': [1, 2, 3],\n",
    "    'column2': [4, 5, 6],\n",
    "    'date_column': pd.to_datetime(['2020-01-01', '2020-01-02', '2020-01-03']),\n",
    "    'irrelevant_column': ['x', 'y', 'z']\n",
    "})\n",
    "\n",
    "# Run the function with a test case\n",
    "keep_columns = ['column1', 'column2']\n",
    "new_df = remove_column(df, keep_columns, include_date_columns=True)\n",
    "\n",
    "# Test assertions\n",
    "assert 'column1' in new_df.columns, \"column1 is not in the DataFrame\"\n",
    "assert 'column2' in new_df.columns, \"column2 is not in the DataFrame\"\n",
    "assert 'date_column' not in new_df.columns, \"date_column should not be retained in the DataFrame based on the current function implementation\"\n",
    "assert 'irrelevant_column' not in new_df.columns, \"irrelevant_column should have been removed from the DataFrame\"\n",
    "assert new_df.shape[1] == 2, \"The number of columns in the DataFrame should be two\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
