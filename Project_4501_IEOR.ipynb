{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part0 Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import psycopg2\n",
    "import sqlalchemy as db\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "import contextily as ctx\n",
    "from geoalchemy2 import Geometry\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import unittest\n",
    "import numpy as np\n",
    "from unittest.mock import patch\n",
    "# warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Downloading 311 and Tree data from API\n",
    "- Download 311 and Tree data using API and Python Code\n",
    "- 311 Data set is pretty huge, so we choose to download it seperately into subfiles by year, and merge it at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Function (1) Manually Doanload Data from NYC Open Data\n",
    "def download_data(url, app_token, filename, date_field, start_date, end_date, date_format=\"%Y-%m-%dT%H:%M:%S\", limit=10000):\n",
    "    \"\"\"\n",
    "    Downloads data from the specified NYC Open Data URL within a given date range.\n",
    "\n",
    "    Args:\n",
    "        url (str): The API endpoint for the dataset.\n",
    "        app_token (str): Application token for authenticated access.\n",
    "        filename (str): The name of the file where the data will be saved.\n",
    "        date_field (str): The name of the date field in the dataset.\n",
    "        start_date (datetime): The start date for filtering data.\n",
    "        end_date (datetime): The end date for filtering data.\n",
    "        date_format (str): Format of the date fields, defaults to '%Y-%m-%dT%H:%M:%S'.\n",
    "        limit (int): Number of records to retrieve per request, defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "        None: This function writes the downloaded data to a file and does not return anything.\n",
    "    \"\"\"\n",
    "    \n",
    "    offset = 0\n",
    "    start_date_str = start_date.strftime(date_format) # Format the start date\n",
    "    end_date_str = end_date.strftime(date_format) # Format the end date\n",
    "    # Construct the query for filtering data by date range\n",
    "    date_query = f\"$where={date_field} between '{start_date_str}' and '{end_date_str}'\"\n",
    "    \n",
    "    first_batch = True  # Flag to identify the first batch of data\n",
    "    while True:\n",
    "        # Construct the full URL with necessary query parameters\n",
    "        full_url = f\"{url}?$$app_token={app_token}&{date_query}&$limit={limit}&$offset={offset}\"\n",
    "        response = requests.get(full_url) # Perform the API request\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.text\n",
    "            records_retrieved = data.count('\\n')  # Count the number of lines (records) retrieved\n",
    "\n",
    "            if first_batch and records_retrieved > 0:  # If this is the first batch and it contains data\n",
    "                with open(filename, 'w') as file:\n",
    "                    file.write(data) # Write data to file, including header\n",
    "                first_batch = False\n",
    "            elif records_retrieved > 1:  # For subsequent batches, skip the header row\n",
    "                with open(filename, 'a') as file:\n",
    "                    file.write(data.split('\\n', 1)[1])  # Append data to file without header\n",
    "\n",
    "            if records_retrieved < limit + 1:   # Check if all records have been retrieved\n",
    "                break\n",
    "            offset += limit # Increment the offset for the next batch\n",
    "        else:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#app Toekn: Application token used for authentication\n",
    "app_token = 'Z8lDMDpdnonlT1RjM5YGII6Ii'\n",
    "#Data URL: Defines the online API URLs for the datasets\n",
    "url_311 = 'https://data.cityofnewyork.us/resource/erm2-nwe9.csv'\n",
    "url_trees = 'https://data.cityofnewyork.us/resource/5rq2-4hqu.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download tree data: Initiates the download of tree data\n",
    "# date format \"%m/%d/%Y\": Setting the date format to month/day/year\n",
    "download_data(\n",
    "    url=url_trees,\n",
    "    app_token=app_token,  \n",
    "    filename=\"data/tree_data.csv\",\n",
    "    date_field=\"created_at\",  \n",
    "    start_date=datetime(2015, 1, 1),\n",
    "    end_date=datetime(2015, 12, 31),\n",
    "    date_format=\"%m/%d/%Y\",  \n",
    "    limit=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download 311 data from 2015.1.1-2023.9.30\n",
    "#create a new folder to save 311 data by year\n",
    "subfolder_name = \"311_data\"\n",
    "subfolder_path = os.path.join(\"data\", subfolder_name)\n",
    "if not os.path.exists(subfolder_path):\n",
    "    os.makedirs(subfolder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run download data function by year\n",
    "#Year 2015\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2015.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2015, 1, 1, 0, 0),  # This represents 2015-01-01 00:00:00 AM\n",
    "    end_date=datetime(2015, 12, 31, 23, 59, 59),  # This represents 2023-09-30 11:59:59 PM\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Test (1)\n",
    "# Path to the file you expect to exist\n",
    "filename = \"data/311_data/311_data_2015.csv\"\n",
    "# Assert that the file exists\n",
    "assert os.path.exists(filename), \"File does not exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2016\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2016.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2016, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2016, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2017\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2017.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2017, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2017, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2018\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2018.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2018, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2018, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2019\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2019.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2019, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2019, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2020\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2020.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2020, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2020, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2021\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2021.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2021, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2021, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2022\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2022.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2022, 1, 1, 0, 0), \n",
    "    end_date=datetime(2022, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2023\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2023.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2023, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2023, 9, 30, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cleaning & Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Data Description & Columns to keep\n",
    "\n",
    "Prior to developing data cleaning functions, we conducted a thorough examination of each dataset's types and descriptions, as well as the query and visualization requirements of our project. The following list details the columns we have chosen to retain. Each column is accompanied by its description and original data type.\n",
    "\n",
    "##### *Zillow Rent Data Description\n",
    "\n",
    "| Column Name                  | Description                                                                          | Type        |\n",
    "|------------------------------|--------------------------------------------------------------------------------------|-------------|\n",
    "| RegionID                     | Used for pandas, an identifier for the region                                         | Integer     |\n",
    "| RegionName                   | Same as postcode, matches 'Incident Zip' in other datasets, link with latitude and longitude | Integer     |\n",
    "| City                         | Different cities, for later filtering to New York                                     | Object      |\n",
    "| Average Housing Price Columns| Keep all columns related to average housing prices for each region                   | Float64     |\n",
    "\n",
    "\n",
    "##### *311 Data\n",
    "https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9\n",
    "\n",
    "311 Service Requests from 2010 to Present\n",
    "This dataset comprises all 311 Service Requests from 2010 to the present day, updated daily. It provides a comprehensive overview of non-emergency requests and complaints to New York City's 311 service.\n",
    "\n",
    "\n",
    "| Column Name    | Description                                                              | Type        |\n",
    "|----------------|--------------------------------------------------------------------------|-------------|\n",
    "| unique_key     | Unique identifier of a Service Request (SR) in the open data set         | Plain Text  |\n",
    "| created_date   | Date SR was created                                                      | Date & Time |\n",
    "| complaint_type | First level of a hierarchy identifying the topic of the incident or condition | Plain Text  |\n",
    "| incident_zip   | Incident location zip code, provided by geo validation                   | Plain Text  |\n",
    "| latitude       | Geo based Latitude of the incident location                              | Number      |\n",
    "| longitude      | Geo based Longitude of the incident location                             | Number      |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### *Tree Data Description\n",
    "https://data.cityofnewyork.us/Environment/2015-Street-Tree-Census-Tree-Data/uvpi-gqnh\n",
    "\n",
    "\n",
    "2015 Street Tree Census, conducted by volunteers and staff organized by NYC Parks & Recreation and partner organizations. Tree data collected includes tree species, diameter and perception of health. Accompanying blockface data is available indicating status of data collection and data release citywide.\n",
    "\n",
    "\n",
    "| Column Name   | Description                                                                   | Type       |\n",
    "|---------------|-------------------------------------------------------------------------------|------------|\n",
    "| tree_id       | Unique identification number for each tree point                              | Integer     |\n",
    "| status        | Indicates whether the tree is alive, standing dead, or a stump               | Plain Text |\n",
    "| created_at  | Date and time when the tree data was created                                  | Plain Text |\n",
    "| zipcode       | Five-digit zipcode in which tree is located                                  | Integer |\n",
    "| latitude      | Latitude of point, in decimal degrees                                        | Number     |\n",
    "| longitude     | Longitude of point, in decimal degrees                                       | Number     |\n",
    "| health        | Indicates the user's perception of tree health                               | Plain Text |\n",
    "| spc_common    | Common name for species, e.g., \"red maple\"                                   | Plain Text |\n",
    "\n",
    "\n",
    "#### *Zipcode Data Description\n",
    "| Column Name | Description                                | Type    |\n",
    "|-------------|--------------------------------------------|---------|\n",
    "| ZIPCODE     | The postal code corresponding to the area  | Plain Text |\n",
    "| geometry    | Geometrical data representing the area     | Geometry |\n",
    "\n",
    "\n",
    "#### From the 4 datasets, (except for the zillow rent, where we added an index as unique identifier because we long the table), we used id/zipcode as unique identifier in part3, hence, we removed the duplicates in this part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2.1 Functions aiding data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function (2) create a remove_column function for deleting the unnecessary columns\n",
    "\n",
    "def remove_column(df, keep_columns, include_date_columns=False, date_pattern=r'\\d{4}-\\d{2}-\\d{2}'):\n",
    "    \"\"\"\n",
    "    Removes columns from a DataFrame, retaining only the specified columns and optionally any date columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to be modified.\n",
    "    - keep_columns (list of str): A list of column names to retain in the DataFrame.\n",
    "    - include_date_columns (bool, optional): Flag to include columns with date format. Defaults to False.\n",
    "    - date_pattern (str, optional): Regular expression pattern to identify date columns. Used when 'include_date_columns' is True. Default pattern matches 'YYYY-MM-DD'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The modified DataFrame with only the specified columns retained.\n",
    "    \n",
    "    Example:\n",
    "    >>> df = pd.DataFrame(...)\n",
    "    >>> new_df = remove_column(df, ['column1', 'column2'], include_date_columns=True)\n",
    "    \"\"\"\n",
    "    # Combine specified columns with date columns if needed\n",
    "    all_columns_to_keep = keep_columns\n",
    "\n",
    "    # If including date columns, append them to the list of columns to keep\n",
    "    if include_date_columns:\n",
    "        # Identify date columns using the regex pattern\n",
    "        date_columns = df.columns[df.columns.str.contains(date_pattern)]\n",
    "        all_columns_to_keep += date_columns.tolist()\n",
    "\n",
    "    # Return the DataFrame with only the specified columns retained\n",
    "    return df.loc[:, all_columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test (2)\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'column1': [1, 2, 3],\n",
    "    'column2': [4, 5, 6],\n",
    "    'date_column': pd.to_datetime(['2020-01-01', '2020-01-02', '2020-01-03']),\n",
    "    'irrelevant_column': ['x', 'y', 'z']\n",
    "})\n",
    "\n",
    "# Run the function with a test case\n",
    "keep_columns = ['column1', 'column2']\n",
    "new_df = remove_column(df, keep_columns, include_date_columns=True)\n",
    "\n",
    "# Test assertions\n",
    "assert 'column1' in new_df.columns, \"column1 is not in the DataFrame\"\n",
    "assert 'column2' in new_df.columns, \"column2 is not in the DataFrame\"\n",
    "assert 'date_column' not in new_df.columns, \"date_column should not be retained in the DataFrame based on the current function implementation\"\n",
    "assert 'irrelevant_column' not in new_df.columns, \"irrelevant_column should have been removed from the DataFrame\"\n",
    "assert new_df.shape[1] == 2, \"The number of columns in the DataFrame should be two\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function (3),(4) create functions to check valid longitute and latitute\n",
    "def is_valid_latitude(lat):\n",
    "    \"\"\"\n",
    "    Checks if the provided latitude value is valid.\n",
    "\n",
    "    Parameters:\n",
    "    - lat (str): The latitude value to be checked.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the latitude is valid, False otherwise.\n",
    "\n",
    "    A valid latitude is a number between -90 and 90.\n",
    "    Non-numeric values or latitudes outside this range will return False.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Check if latitude is within the valid range\n",
    "        return -90 <= float(lat) <= 90\n",
    "    except ValueError:\n",
    "        # Return False if latitude is not a number\n",
    "        return False\n",
    "\n",
    "def is_valid_longitude(lon):\n",
    "    \"\"\"\n",
    "    Checks if the provided longitude value is valid.\n",
    "\n",
    "    Parameters:\n",
    "    - lon (str): The longitude value to be checked.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the longitude is valid, False otherwise.\n",
    "\n",
    "    A valid longitude is a number between -180 and 180.\n",
    "    Non-numeric values or longitudes outside this range will return False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if longitude is within the valid range\n",
    "        return -180 <= float(lon) <= 180\n",
    "    except ValueError:\n",
    "        # Return False if longitude is not a number\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Test (3) cases for is_valid_latitude\n",
    "assert is_valid_latitude(\"45\"), \"45 should be a valid latitude\"\n",
    "assert is_valid_latitude(\"-90\"), \"-90 should be a valid latitude\"\n",
    "assert is_valid_latitude(\"90\"), \"90 should be a valid latitude\"\n",
    "assert not is_valid_latitude(\"100\"), \"100 should not be a valid latitude\"\n",
    "assert not is_valid_latitude(\"-91\"), \"-91 should not be a valid latitude\"\n",
    "assert not is_valid_latitude(\"not_a_number\"), \"'not_a_number' should not be a valid latitude\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Test (4) cases for is_valid_longitude\n",
    "assert is_valid_longitude(\"90\"), \"90 should be a valid longitude\"\n",
    "assert is_valid_longitude(\"-180\"), \"-180 should be a valid longitude\"\n",
    "assert is_valid_longitude(\"180\"), \"180 should be a valid longitude\"\n",
    "assert not is_valid_longitude(\"190\"), \"190 should not be a valid longitude\"\n",
    "assert not is_valid_longitude(\"-181\"), \"-181 should not be a valid longitude\"\n",
    "assert not is_valid_longitude(\"not_a_number\"), \"'not_a_number' should not be a valid longitude\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2.2 Functions performing data cleaning and filtering for each datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. zipcode\n",
    "# Function (5) Create a clean and filter function for zipcode data\n",
    "def process_zipcode_shapefile(shapefile_path):\n",
    "    \"\"\"\n",
    "    Processes and cleans the zipcode shapefile to prepare it for analysis.\n",
    "\n",
    "    Args:\n",
    "    - shapefile_path (str): Path to the zipcode shapefile.\n",
    "\n",
    "    Returns:\n",
    "    - gpd.GeoDataFrame: A GeoDataFrame containing the cleaned and processed zipcode data.\n",
    "\n",
    "    This function performs several steps to clean the data, including selecting essential columns,\n",
    "    removing duplicates, filtering out missing values, renaming and reformatting columns, \n",
    "    and adjusting geometries to a common Coordinate Reference System (CRS).\n",
    "    \"\"\"\n",
    "    # Step 1: Reading the shapefile\n",
    "    zipcode_gdf = gpd.read_file(shapefile_path)\n",
    "    \n",
    "    # Step 2: Selecting essential columns\n",
    "    columns_required = ['ZIPCODE', 'geometry']\n",
    "    zipcode_gdf = zipcode_gdf[columns_required]\n",
    "    \n",
    "    \n",
    "    # Step 3: Remove duplicates\n",
    "    zipcode_gdf.drop_duplicates(subset=['ZIPCODE'], inplace=True)\n",
    "    \n",
    "    \n",
    "    # Step 4: Filtering out rows with missing values in critical columns\n",
    "    zipcode_gdf.dropna(subset=['ZIPCODE', 'geometry'], inplace=True)\n",
    "    \n",
    "    \n",
    "    # Step 5: Renaming and reformatting the columns\n",
    "    # Renaming 'ZIPCODE' to 'zipcode' for consistency\n",
    "    zipcode_gdf.rename(columns={'ZIPCODE': 'zipcode'}, inplace=True)\n",
    "    \n",
    "        \n",
    "    # Converting the 'zipcode' to string format for standardization\n",
    "    zipcode_gdf['zipcode'] = zipcode_gdf['zipcode'].astype(int)  # Convert zipcode to integer\n",
    "    \n",
    "    \n",
    "    # check if it lasts with 5 digits\n",
    "    zipcode_gdf = zipcode_gdf[zipcode_gdf['zipcode'].apply(lambda x: str(x).isdigit() and len(str(x)) == 5)]\n",
    "\n",
    "    # Step 6: Adjusting geometries to a common CRS\n",
    "    # Transforming geometries to the web mercator projection for mapping compatibility\n",
    "    common_crs = \"EPSG:3857\"\n",
    "    zipcode_gdf.to_crs(common_crs, inplace=True)\n",
    "    \n",
    "    # Ensuring all column names are in lowercase\n",
    "    zipcode_gdf.columns = map(str.lower, zipcode_gdf.columns)\n",
    "    \n",
    "    return zipcode_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 311 Data\n",
    "# Function (6)\n",
    "def clean_filter_311(filename, nyc_zipcodes):\n",
    "    \"\"\"\n",
    "    Cleans and filters 311 complaint data to retain only relevant information and valid entries.\n",
    "\n",
    "    Args:\n",
    "    - filename (str): Path to the CSV file containing 311 data.\n",
    "    - nyc_zipcodes (list of int): List of valid NYC zip codes for filtering.\n",
    "\n",
    "    Returns:\n",
    "    - gpd.GeoDataFrame: A GeoDataFrame containing the cleaned and filtered 311 complaint data.\n",
    "\n",
    "    The function includes data loading, column selection, data validation, normalization of column names and types, \n",
    "    and transformation of data into a geospatial format.\n",
    "    \"\"\"\n",
    "    # Step 0: Load data\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Step 1: Select necessary columns\n",
    "    columns_to_keep_311 = ['unique_key', 'created_date', 'complaint_type', 'incident_zip', 'latitude', 'longitude']\n",
    "    df = df[columns_to_keep_311]\n",
    "\n",
    "    # Step 2: Remove invalid and duplicate data\n",
    "    # Remove duplicates and confirm unique_key\n",
    "    df = df.drop_duplicates(subset='unique_key')\n",
    "    df = df[df['unique_key'].notna() & df['unique_key'].apply(lambda x: str(x).isdigit())]\n",
    "\n",
    "    # Remove rows with NaN in critical columns\n",
    "    df = df.dropna(subset=['incident_zip', 'latitude', 'longitude', 'created_date'])\n",
    "\n",
    "    # Check if latitude and longitude are valid\n",
    "    df = df[df['latitude'].apply(is_valid_latitude) & df['longitude'].apply(is_valid_longitude)]\n",
    "\n",
    "    # Step 3: Normalize Column Names, Column Types\n",
    "    \n",
    "    # Standardize 'incident_zip' to integer and filter by NYC zipcodes\n",
    "    df['incident_zip'] = df['incident_zip'].astype(int)\n",
    "    df = df[df['incident_zip'].isin(nyc_zipcodes)]\n",
    "\n",
    "    # Rename and reformat columns for consistency\n",
    "    df.rename(columns={'unique_key': 'complaint_id', 'created_date': 'date', 'incident_zip': 'zipcode'}, inplace=True)\n",
    "    df['complaint_id'] = df['complaint_id'].astype(int)\n",
    "\n",
    "    # Convert 'date' column to datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Drop rows with null values post transformations\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Step 4: Normalize the SRID of any geometry\n",
    "    \n",
    "    # Convert DataFrame to GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['longitude'], df['latitude']))\n",
    "    gdf.set_crs(\"EPSG:4326\", inplace=True)\n",
    "    gdf.to_crs(\"EPSG:3857\", inplace=True)\n",
    "\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Function (7) to clean and merge 311 data\n",
    "def clean_merge_311_data(folder_path, file_prefix, years,nyc_zipcodes):\n",
    "    \"\"\"\n",
    "    Cleans individual 311 data files for specified years and merges them into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - folder_path (str): Path to the folder containing 311 data files.\n",
    "    - file_prefix (str): Prefix of the 311 data filenames.\n",
    "    - years (list of int): List of years for which the 311 data needs to be cleaned and merged.\n",
    "    - nyc_zipcodes (list of int): List of valid NYC zip codes for filtering.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A merged DataFrame containing cleaned and consolidated 311 data for the specified years.\n",
    "\n",
    "    The function iteratively cleans each year's data file using 'clean_filter_311' and then merges them into one DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # List to hold cleaned DataFrames for each year\n",
    "    cleaned_dfs = []\n",
    "    \n",
    "    # Iterate over each year, clean the corresponding 311 data file, and add it to the list\n",
    "    for year in years:\n",
    "        filename = f\"{folder_path}/{file_prefix}{year}.csv\"\n",
    "        cleaned_df = clean_filter_311(filename,nyc_zipcodes)\n",
    "        cleaned_dfs.append(cleaned_df)\n",
    "    \n",
    "    # # Merge all cleaned DataFrames into a single DataFrame\n",
    "    merged_df = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Function (8) to clean and filter tree data\n",
    "def clean_filter_tree(filename, nyc_zipcodes):\n",
    "    \"\"\"\n",
    "    Cleans and filters tree data, retaining only the relevant information and valid entries.\n",
    "\n",
    "    Args:\n",
    "    - filename (str): Path to the CSV file containing tree data.\n",
    "    - nyc_zipcodes (list of int): List of valid NYC zip codes for filtering.\n",
    "\n",
    "    Returns:\n",
    "    - gpd.GeoDataFrame: A GeoDataFrame containing the cleaned and filtered tree data.\n",
    "\n",
    "    This function includes loading data, selecting necessary columns, removing invalid and duplicate data,\n",
    "    and transforming the data into a geospatial format.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Select the necessary columns for analysis\n",
    "    columns_to_keep_tree = ['tree_id', 'status', 'zipcode', 'latitude', 'longitude', 'health', 'spc_common', 'created_at']\n",
    "    df = df[columns_to_keep_tree]\n",
    "\n",
    "    # Remove duplicate entries and ensure tree IDs are valid (tree id as uid)\n",
    "    df = df.drop_duplicates(subset='tree_id')\n",
    "    df = df[df['tree_id'].notna() & df['tree_id'].apply(lambda x: str(x).isdigit())]\n",
    "\n",
    "    # Drop rows with missing values in critical columns before type conversions\n",
    "    df = df.dropna(subset=['zipcode', 'latitude', 'longitude', 'created_at'])\n",
    "\n",
    "    # Standardize 'zipcode' to integer and filter by NYC zipcodes\n",
    "    df['zipcode'] = df['zipcode'].astype(int)\n",
    "    df = df[df['zipcode'].isin(nyc_zipcodes)]\n",
    "\n",
    "    # Validate latitude and longitude values\n",
    "    df = df[df['latitude'].apply(is_valid_latitude) & df['longitude'].apply(is_valid_longitude)]\n",
    "\n",
    "    # Rename and reformat columns for consistency\n",
    "    df.rename(columns={'created_at': 'date', 'spc_common': 'species'}, inplace=True)\n",
    "\n",
    "    # Convert 'date' column to datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Convert to GeoDataFrame and normalize SRID\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['longitude'], df['latitude']))\n",
    "    gdf.set_crs(\"EPSG:4326\", inplace=True)\n",
    "    gdf.to_crs(\"EPSG:3857\", inplace=True)\n",
    "\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Function (9) to clean and filter zillow rent data\n",
    "def clean_filter_rent(filename, nyc_zipcodes):\n",
    "    \"\"\"\n",
    "    Cleans and filters Zillow rent data, retaining only the relevant information for NYC regions.\n",
    "\n",
    "    Args:\n",
    "    - filename (str): Path to the CSV file containing Zillow rent data.\n",
    "    - nyc_zipcodes (list of int): List of valid NYC zip codes for filtering.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing the cleaned and filtered Zillow rent data.\n",
    "\n",
    "    This function includes loading the data, dropping unnecessary columns, renaming columns, \n",
    "    and transforming the data format for analysis.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Drop 'City' column\n",
    "    df = df.drop(columns=['City'])\n",
    "    \n",
    "    # Rename 'RegionID' and 'RegionName' columns for standardization\n",
    "    df.rename(columns={'RegionID': 'region_id', 'RegionName': 'zipcode'}, inplace=True)\n",
    "    \n",
    "    # Remove duplicate entries based on 'region_id'\n",
    "    df = df.drop_duplicates(subset='region_id')\n",
    "    \n",
    "    # Drop rows with NA values in 'zipcode' column\n",
    "    df = df.dropna(subset=['zipcode'])\n",
    "\n",
    "    # Ensure that the 'zipcode' is an integer format for consistency\n",
    "    df['zipcode'] = df['zipcode'].astype(int)\n",
    "\n",
    "    # Filter the DataFrame to include only rows with NYC zip codes\n",
    "    df = df[df['zipcode'].isin(nyc_zipcodes)]\n",
    "    \n",
    "    # Transpose the dataframe: each date becomes a row\n",
    "    id_vars = ['region_id', 'zipcode']\n",
    "    value_vars = [col for col in df.columns if col not in id_vars and col.startswith('20')]\n",
    "    df_melted = df.melt(id_vars=id_vars, value_vars=value_vars, var_name='date', value_name='rent')\n",
    "    \n",
    "    # Add 'rent_id' column as an identifier to uniquely identify each rent entry\n",
    "    df_melted.insert(0, 'rent_id', range(1, 1 + len(df_melted)))\n",
    "    \n",
    "    # Group by 'zipcode' and forward fill followed by backward fill missing data, used for queries\n",
    "    df_melted['rent'] = df_melted.groupby('zipcode')['rent'].apply(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n",
    "\n",
    "    # Filter out rows with NaN values in any column\n",
    "    df_melted = df_melted.dropna()\n",
    "    \n",
    "    return df_melted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#311 data input arguments\n",
    "folder_path = \"data/311_data\"\n",
    "file_prefix = \"311_data_\"\n",
    "years = ['2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
    "\n",
    "# Function (10) load all data\n",
    "def load_all_data():\n",
    "    \"\"\"\n",
    "    Loads and processes all datasets: Zipcode, 311, Tree, and Zillow Rent data.\n",
    "\n",
    "    This function first processes the NYC zipcode data to create a list of valid NYC zipcodes. \n",
    "    It then uses this list to filter and process the 311, Tree, and Zillow Rent datasets.\n",
    "\n",
    "    Returns:\n",
    "    Tuple: Containing GeoDataFrames for Zipcode and 311 data, and DataFrames for Tree and Zillow Rent data.\n",
    "    \"\"\"\n",
    "    # Process the zipcode shapefile to create a GeoDataFrame of NYC zipcodes\n",
    "    geodf_zipcode_data = process_zipcode_shapefile(\"data/nyc_zipcodes/ZIP_CODE_040114.shp\")\n",
    "    \n",
    "    # Extract a list of valid NYC zipcodes from the processed zipcode data\n",
    "    nyc_zipcodes = geodf_zipcode_data['zipcode'].tolist()\n",
    "    \n",
    "     # Clean and merge 311 data for the specified years using the list of NYC zipcodes\n",
    "    geodf_311_data = clean_merge_311_data(folder_path, file_prefix, years,nyc_zipcodes)\n",
    "    \n",
    "    # Clean and filter tree data using the list of NYC zipcodes\n",
    "    geodf_tree_data = clean_filter_tree('data/tree_data.csv',nyc_zipcodes)\n",
    "    \n",
    "    # Clean and filter Zillow Rent data using the list of NYC zipcodes\n",
    "    df_zillow_data = clean_filter_rent('data/zillow_rent_data.csv',nyc_zipcodes)\n",
    "    \n",
    "    # Return all processed datasets\n",
    "    return (\n",
    "        geodf_zipcode_data,\n",
    "        geodf_311_data,\n",
    "        geodf_tree_data,\n",
    "        df_zillow_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the load_all_data function to load and process all datasets\n",
    "# The function returns four datasets: Zipcode, 311, Tree, and Zillow Rent data\n",
    "# Each dataset is stored in a separate variable for further analysis\n",
    "\n",
    "geodf_zipcode_data, geodf_311_data, geodf_tree_data, df_zillow_data = load_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test (5) for zipcode data\n",
    "assert isinstance(geodf_zipcode_data, gpd.GeoDataFrame), \"The result should be a GeoDataFrame\"\n",
    "assert 'zipcode' in geodf_zipcode_data.columns, \"zipcode column is missing\"\n",
    "assert geodf_zipcode_data['zipcode'].dtype == int, \"zipcode column should be of integer type\"\n",
    "assert len(geodf_zipcode_data['zipcode'].unique()) == len(geodf_zipcode_data), \"There should be no duplicate zipcodes\"\n",
    "assert geodf_zipcode_data.isnull().sum().sum() == 0, \"There should be no missing values\"\n",
    "assert geodf_zipcode_data.crs.to_string() == \"EPSG:3857\", \"CRS should be EPSG:3857\"\n",
    "assert all(len(str(z)) == 5 for z in geodf_zipcode_data['zipcode']), \"All zipcodes should have 5 digits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test (6) for 311 data\n",
    "assert isinstance(geodf_311_data, gpd.GeoDataFrame), \"The result should be a GeoDataFrame\"\n",
    "assert 'zipcode' in geodf_311_data.columns, \"zipcode column is missing in 311 data\"\n",
    "assert geodf_311_data['zipcode'].dtype == int, \"zipcode column should be of integer type in 311 data\"\n",
    "assert geodf_311_data.isnull().sum().sum() == 0, \"There should be no missing values in 311 data\"\n",
    "assert geodf_311_data.crs.to_string() == \"EPSG:3857\", \"CRS of 311 data should be EPSG:3857\"\n",
    "assert all(len(str(z)) == 5 for z in geodf_311_data['zipcode']), \"All zipcodes in 311 data should have 5 digits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test (7) for clean_merge function\n",
    "assert isinstance(geodf_311_data, pd.DataFrame), \"The result should be a DataFrame\"\n",
    "assert 'zipcode' in geodf_311_data.columns, \"zipcode column is missing in merged 311 data\"\n",
    "assert geodf_311_data['zipcode'].dtype == int, \"zipcode column should be of integer type in merged 311 data\"\n",
    "assert geodf_311_data.isnull().sum().sum() == 0, \"There should be no missing values in merged 311 data\"\n",
    "assert all(len(str(z)) == 5 for z in geodf_311_data['zipcode']), \"All zipcodes in merged 311 data should have 5 digits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test (8) for tree data function\n",
    "assert isinstance(geodf_tree_data, gpd.GeoDataFrame), \"The result should be a GeoDataFrame\"\n",
    "assert 'zipcode' in geodf_tree_data.columns, \"zipcode column is missing in tree data\"\n",
    "assert geodf_tree_data['zipcode'].dtype == int, \"zipcode column should be of integer type in tree data\"\n",
    "assert geodf_tree_data.crs.to_string() == \"EPSG:3857\", \"CRS of tree data should be EPSG:3857\"\n",
    "assert all(len(str(z)) == 5 for z in geodf_tree_data['zipcode']), \"All zipcodes in tree data should have 5 digits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test (9) for zillow data function\n",
    "assert isinstance(df_zillow_data, pd.DataFrame), \"The result should be a DataFrame\"\n",
    "assert 'zipcode' in df_zillow_data.columns, \"zipcode column is missing in Zillow rent data\"\n",
    "assert df_zillow_data['zipcode'].dtype == int, \"zipcode column should be of integer type in Zillow rent data\"\n",
    "assert df_zillow_data.isnull().sum().sum() == 0, \"There should be no missing values in Zillow rent data\"\n",
    "assert 'date' in df_zillow_data.columns, \"date column is missing in Zillow rent data\"\n",
    "assert 'rent' in df_zillow_data.columns, \"rent column is missing in Zillow rent data\"\n",
    "assert 'rent_id' in df_zillow_data.columns, \"rent_id column is missing in Zillow rent data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test (10) for load all data\n",
    "assert isinstance(geodf_zipcode_data, gpd.GeoDataFrame), \"Zipcode data should be a GeoDataFrame\"\n",
    "assert isinstance(geodf_311_data, gpd.GeoDataFrame), \"311 data should be a GeoDataFrame\"\n",
    "assert isinstance(geodf_tree_data, gpd.GeoDataFrame), \"Tree data should be a GeoDataFrame\"\n",
    "assert isinstance(df_zillow_data, pd.DataFrame), \"Zillow Rent data should be a DataFrame\"\n",
    "\n",
    "\n",
    "assert not geodf_zipcode_data.empty, \"Zipcode data should not be empty\"\n",
    "assert not geodf_311_data.empty, \"311 data should not be empty\"\n",
    "assert not geodf_tree_data.empty, \"Tree data should not be empty\"\n",
    "assert not df_zillow_data.empty, \"Zillow Rent data should not be empty\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check each data frame to check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zipcode</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11436</td>\n",
       "      <td>POLYGON ((-8216029.470 4965682.769, -8216011.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11213</td>\n",
       "      <td>POLYGON ((-8230673.455 4965216.008, -8230392.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11212</td>\n",
       "      <td>POLYGON ((-8226837.796 4963911.170, -8226758.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11225</td>\n",
       "      <td>POLYGON ((-8232963.912 4963884.338, -8232717.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11218</td>\n",
       "      <td>POLYGON ((-8234534.400 4960940.544, -8234516.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   zipcode                                           geometry\n",
       "0    11436  POLYGON ((-8216029.470 4965682.769, -8216011.9...\n",
       "1    11213  POLYGON ((-8230673.455 4965216.008, -8230392.3...\n",
       "2    11212  POLYGON ((-8226837.796 4963911.170, -8226758.2...\n",
       "3    11225  POLYGON ((-8232963.912 4963884.338, -8232717.3...\n",
       "4    11218  POLYGON ((-8234534.400 4960940.544, -8234516.0..."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first 5 entries about each dataframe\n",
    "geodf_zipcode_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 247 entries, 0 to 262\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   zipcode   247 non-null    int64   \n",
      " 1   geometry  247 non-null    geometry\n",
      "dtypes: geometry(1), int64(1)\n",
      "memory usage: 5.8 KB\n"
     ]
    }
   ],
   "source": [
    "geodf_zipcode_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 23026041 entries, 0 to 23026040\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Dtype   \n",
      "---  ------          -----   \n",
      " 0   complaint_id    int64   \n",
      " 1   date            object  \n",
      " 2   complaint_type  object  \n",
      " 3   zipcode         int64   \n",
      " 4   latitude        float64 \n",
      " 5   longitude       float64 \n",
      " 6   geometry        geometry\n",
      "dtypes: float64(2), geometry(1), int64(2), object(2)\n",
      "memory usage: 1.2+ GB\n"
     ]
    }
   ],
   "source": [
    "geodf_311_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complaint_id</th>\n",
       "      <th>date</th>\n",
       "      <th>complaint_type</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32310363</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>Noise - Street/Sidewalk</td>\n",
       "      <td>10034</td>\n",
       "      <td>40.865682</td>\n",
       "      <td>-73.923501</td>\n",
       "      <td>POINT (-8229126.484 4992549.863)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32309934</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>Blocked Driveway</td>\n",
       "      <td>11105</td>\n",
       "      <td>40.775945</td>\n",
       "      <td>-73.915094</td>\n",
       "      <td>POINT (-8228190.619 4979349.608)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32306007</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>Noise - Residential</td>\n",
       "      <td>10302</td>\n",
       "      <td>40.632882</td>\n",
       "      <td>-74.132033</td>\n",
       "      <td>POINT (-8252340.140 4958341.738)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32309159</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>Blocked Driveway</td>\n",
       "      <td>10458</td>\n",
       "      <td>40.870325</td>\n",
       "      <td>-73.888525</td>\n",
       "      <td>POINT (-8225232.939 4993233.335)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32309493</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>Noise - Residential</td>\n",
       "      <td>10002</td>\n",
       "      <td>40.710478</td>\n",
       "      <td>-73.986571</td>\n",
       "      <td>POINT (-8236147.417 4969730.548)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   complaint_id        date           complaint_type  zipcode   latitude  \\\n",
       "0      32310363  2015-12-31  Noise - Street/Sidewalk    10034  40.865682   \n",
       "1      32309934  2015-12-31         Blocked Driveway    11105  40.775945   \n",
       "2      32306007  2015-12-31      Noise - Residential    10302  40.632882   \n",
       "3      32309159  2015-12-31         Blocked Driveway    10458  40.870325   \n",
       "4      32309493  2015-12-31      Noise - Residential    10002  40.710478   \n",
       "\n",
       "   longitude                          geometry  \n",
       "0 -73.923501  POINT (-8229126.484 4992549.863)  \n",
       "1 -73.915094  POINT (-8228190.619 4979349.608)  \n",
       "2 -74.132033  POINT (-8252340.140 4958341.738)  \n",
       "3 -73.888525  POINT (-8225232.939 4993233.335)  \n",
       "4 -73.986571  POINT (-8236147.417 4969730.548)  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geodf_311_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 682853 entries, 0 to 683787\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count   Dtype   \n",
      "---  ------     --------------   -----   \n",
      " 0   tree_id    682853 non-null  int64   \n",
      " 1   status     682853 non-null  object  \n",
      " 2   zipcode    682853 non-null  int64   \n",
      " 3   latitude   682853 non-null  float64 \n",
      " 4   longitude  682853 non-null  float64 \n",
      " 5   health     651240 non-null  object  \n",
      " 6   species    651237 non-null  object  \n",
      " 7   date       682853 non-null  object  \n",
      " 8   geometry   682853 non-null  geometry\n",
      "dtypes: float64(2), geometry(1), int64(2), object(4)\n",
      "memory usage: 52.1+ MB\n"
     ]
    }
   ],
   "source": [
    "geodf_tree_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree_id</th>\n",
       "      <th>status</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>health</th>\n",
       "      <th>species</th>\n",
       "      <th>date</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180683</td>\n",
       "      <td>Alive</td>\n",
       "      <td>11375</td>\n",
       "      <td>40.723092</td>\n",
       "      <td>-73.844215</td>\n",
       "      <td>Fair</td>\n",
       "      <td>red maple</td>\n",
       "      <td>2015-08-27</td>\n",
       "      <td>POINT (-8220300.436 4971583.163)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200540</td>\n",
       "      <td>Alive</td>\n",
       "      <td>11357</td>\n",
       "      <td>40.794111</td>\n",
       "      <td>-73.818679</td>\n",
       "      <td>Fair</td>\n",
       "      <td>pin oak</td>\n",
       "      <td>2015-09-03</td>\n",
       "      <td>POINT (-8217457.809 4982020.303)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>204026</td>\n",
       "      <td>Alive</td>\n",
       "      <td>11211</td>\n",
       "      <td>40.717581</td>\n",
       "      <td>-73.936608</td>\n",
       "      <td>Good</td>\n",
       "      <td>honeylocust</td>\n",
       "      <td>2015-09-05</td>\n",
       "      <td>POINT (-8230585.520 4970773.712)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204337</td>\n",
       "      <td>Alive</td>\n",
       "      <td>11211</td>\n",
       "      <td>40.713537</td>\n",
       "      <td>-73.934456</td>\n",
       "      <td>Good</td>\n",
       "      <td>honeylocust</td>\n",
       "      <td>2015-09-05</td>\n",
       "      <td>POINT (-8230346.012 4970179.889)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189565</td>\n",
       "      <td>Alive</td>\n",
       "      <td>11215</td>\n",
       "      <td>40.666778</td>\n",
       "      <td>-73.975979</td>\n",
       "      <td>Good</td>\n",
       "      <td>American linden</td>\n",
       "      <td>2015-08-30</td>\n",
       "      <td>POINT (-8234968.356 4963315.009)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tree_id status  zipcode   latitude  longitude health          species  \\\n",
       "0   180683  Alive    11375  40.723092 -73.844215   Fair        red maple   \n",
       "1   200540  Alive    11357  40.794111 -73.818679   Fair          pin oak   \n",
       "2   204026  Alive    11211  40.717581 -73.936608   Good      honeylocust   \n",
       "3   204337  Alive    11211  40.713537 -73.934456   Good      honeylocust   \n",
       "4   189565  Alive    11215  40.666778 -73.975979   Good  American linden   \n",
       "\n",
       "         date                          geometry  \n",
       "0  2015-08-27  POINT (-8220300.436 4971583.163)  \n",
       "1  2015-09-03  POINT (-8217457.809 4982020.303)  \n",
       "2  2015-09-05  POINT (-8230585.520 4970773.712)  \n",
       "3  2015-09-05  POINT (-8230346.012 4970179.889)  \n",
       "4  2015-08-30  POINT (-8234968.356 4963315.009)  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geodf_tree_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15330 entries, 0 to 15329\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   rent_id    15330 non-null  int64  \n",
      " 1   region_id  15330 non-null  int64  \n",
      " 2   zipcode    15330 non-null  int64  \n",
      " 3   date       15330 non-null  object \n",
      " 4   rent       15330 non-null  float64\n",
      "dtypes: float64(1), int64(3), object(1)\n",
      "memory usage: 599.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_zillow_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rent_id</th>\n",
       "      <th>region_id</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>date</th>\n",
       "      <th>rent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>62093</td>\n",
       "      <td>11385</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>2087.527084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>62019</td>\n",
       "      <td>11208</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>2334.735073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>62046</td>\n",
       "      <td>11236</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>2285.460026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>61807</td>\n",
       "      <td>10467</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>1453.434755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>62085</td>\n",
       "      <td>11373</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>1852.158106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rent_id  region_id  zipcode        date         rent\n",
       "0        1      62093    11385  2015-01-31  2087.527084\n",
       "1        2      62019    11208  2015-01-31  2334.735073\n",
       "2        3      62046    11236  2015-01-31  2285.460026\n",
       "3        4      61807    10467  2015-01-31  1453.434755\n",
       "4        5      62085    11373  2015-01-31  1852.158106"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zillow_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2: Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Create Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createdb: error: database creation failed: ERROR:  database \"FINAL_PROJECT_4511\" already exists\n",
      "NOTICE:  extension \"postgis\" already exists, skipping\n",
      "CREATE EXTENSION\n"
     ]
    }
   ],
   "source": [
    "!createdb FINAL_PROJECT_4511\n",
    "!psql --dbname FINAL_PROJECT_4511 -c 'CREATE EXTENSION if NOT EXISTS postgis;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'postgres'\n",
    "password = '1'\n",
    "host = 'localhost'\n",
    "port = '5432'\n",
    "database= 'FINAL_PROJECT_4511'\n",
    "DB_URL = f'postgresql://{user}:{password}@{host}:{port}/{database}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to the PostgreSQL database\n",
    "conn=psycopg2.connect(DB_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an engine using SQLAlchemy\n",
    "engine = db.create_engine(DB_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Create Tables\n",
    "\n",
    "In this section, we focused on the key steps of establishing database tables and preparing our datasets for SQL upload:\n",
    "\n",
    "1. **Database Table Creation**: \n",
    "   - Created tables in the PostgreSQL database for storing Zipcode, NYC 311, Tree, and Zillow Rent data. \n",
    "   - Structured each table according to a specific schema to align with the data's format and requirements.\n",
    "\n",
    "2. **SQL Schema File Generation**: \n",
    "   - Generated a `schema.sql` file with SQL commands for table creation, ensuring a consistent database schema.\n",
    "\n",
    "3. **Data Conversion for SQL Upload**: \n",
    "   - Developed a function, `convert_geodf_for_sql`, to format GeoDataFrames for SQL. \n",
    "   - Adjusted the CRS to EPSG:3857 and converted geometries to WKT format for SQL compatibility.\n",
    "\n",
    "4. **Geometry Type and SRID Specification**: \n",
    "   - Specified the geometry type (POINT for 311 and Tree data; POLYGON for Zipcode data) and SRID (3857) for each dataset.\n",
    "\n",
    "5. **Uploading Data to SQL Database**: \n",
    "   - Used SQLAlchemy to upload processed data to the respective SQL tables, avoiding the creation of extra indices for efficiency.\n",
    "\n",
    "These steps effectively prepared and integrated our datasets into the database, laying a solid groundwork for further data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tables for storing four datas\n",
    "#Using SQL\n",
    "\n",
    "#schema.sql\n",
    "ZIPCODE_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS zip (\n",
    "    zipcode INT PRIMARY KEY, \n",
    "    geometry GEOMETRY (POLYGON,3857)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "NYC_311_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS complaint(\n",
    "    complaint_id INT PRIMARY KEY,\n",
    "    date DATE,\n",
    "    complaint_type VARCHAR(200),\n",
    "    zipcode INT,\n",
    "    latitude FLOAT,\n",
    "    longitude FLOAT,\n",
    "    geometry GEOMETRY (POINT, 3857)\n",
    ");\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "NYC_TREE_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS tree (\n",
    "    tree_id INT PRIMARY KEY,\n",
    "    status VARCHAR(60),\n",
    "    zipcode INT,\n",
    "    latitude FLOAT,\n",
    "    longitude FLOAT,\n",
    "    health VARCHAR(60),\n",
    "    species VARCHAR(60),\n",
    "    date DATE,\n",
    "    geometry GEOMETRY (POINT,3857)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "ZILLOW_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS zillow (\n",
    "    rent_id INT PRIMARY KEY,\n",
    "    region_id INT,\n",
    "    zipcode INT,\n",
    "    date DATE,\n",
    "    rent FLOAT\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_SCHEMA_FILE = \"schema.sql\"\n",
    "# create that required schema.sql file\n",
    "with open(DB_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(ZIPCODE_SCHEMA)\n",
    "    f.write(NYC_311_SCHEMA)\n",
    "    f.write(NYC_TREE_SCHEMA)\n",
    "    f.write(ZILLOW_SCHEMA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table using query\n",
    "with engine.connect() as connection:\n",
    "    # Open the SQL schema file that contains table creation statements\n",
    "    with open(DB_SCHEMA_FILE, 'r') as schema_file:\n",
    "        # Read the SQL statements from the file\n",
    "        schema_sql = schema_file.read()\n",
    "        # Execute the SQL statements to create tables in the database\n",
    "        connection.execute(db.text(schema_sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function (11) Create a function to convert GeoDataFrame for SQL upload\n",
    "def convert_geodf_for_sql(geodf, geometry_type='POINT', srid=3857):\n",
    "    \"\"\"\n",
    "    Converts a GeoDataFrame to a format suitable for uploading to SQL.\n",
    "\n",
    "    Args:\n",
    "    - geodf (GeoDataFrame): The GeoDataFrame to convert.\n",
    "    - geometry_type (str, optional): The geometry type (e.g., 'POINT', 'POLYGON'). Defaults to 'POINT'.\n",
    "    - srid (int, optional): The SRID for the geometry. Defaults to 3857 (Web Mercator).\n",
    "\n",
    "    Returns:\n",
    "    Tuple: A DataFrame suitable for SQL upload and a dictionary defining the geometry column data type.\n",
    "    \"\"\"\n",
    "    # Create a copy of the GeoDataFrame to avoid modifying the original\n",
    "    geodf_for_sql = geodf.copy()\n",
    "    # Ensure the CRS is set to EPSG:3857\n",
    "    if geodf_for_sql.crs.to_epsg() != 3857:\n",
    "        geodf_for_sql = geodf_for_sql.to_crs(epsg=3857)\n",
    "    # Convert geometry to Well-Known Text (WKT) format for SQL compatibility\n",
    "    geodf_for_sql['geometry'] = geodf_for_sql['geometry'].apply(lambda x: x.wkt)\n",
    "    # Define the data type for the geometry column for SQL\n",
    "    dtype = {'geometry': Geometry(geometry_type=geometry_type, srid=srid)}\n",
    "    return geodf_for_sql, dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test (11)\n",
    "# Create a sample GeoDataFrame for testing\n",
    "data = {'id': [1, 2, 3], 'geometry': [Point(1, 1), Point(2, 2), Point(3, 3)]}\n",
    "geodf_sample = gpd.GeoDataFrame(data, crs=\"EPSG:4326\")\n",
    "\n",
    "# Call the function with the sample GeoDataFrame\n",
    "df_for_sql, dtype_dict = convert_geodf_for_sql(geodf_sample)\n",
    "assert isinstance(df_for_sql, pd.DataFrame), \"The result should be a DataFrame\"\n",
    "assert 'geometry' in df_for_sql.columns, \"geometry column is missing\"\n",
    "assert isinstance(dtype_dict, dict), \"The dtype should be a dictionary\"\n",
    "assert 'geometry' in dtype_dict, \"geometry key should be in the dtype dictionary\"\n",
    "assert dtype_dict['geometry'].geometry_type == 'POINT', \"The geometry type should be 'POINT'\"\n",
    "assert dtype_dict['geometry'].srid == 3857, \"The SRID should be 3857\"\n",
    "assert all(isinstance(geom, str) for geom in df_for_sql['geometry']), \"All geometry values should be strings (WKT format)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GeoDataFrames into a format suitable for SQL upload, specifying the geometry type and SRID\n",
    "\n",
    "# Convert the zipcode GeoDataFrame, specifying 'POLYGON' as the geometry type\n",
    "geodf_zipcode_data_for_sql, zip_dtype = convert_geodf_for_sql(geodf_zipcode_data,'POLYGON',3857)\n",
    "# Convert the 311 and tree data GeoDataFrame\n",
    "geodf_311_data_for_sql, complaint_dtype = convert_geodf_for_sql(geodf_311_data)\n",
    "geodf_tree_data_for_sql, tree_dtype = convert_geodf_for_sql(geodf_tree_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the prepared DataFrames to the SQL database without creating extra index as uid\n",
    "geodf_zipcode_data_for_sql.to_sql('zip', engine, if_exists='append', index=False, dtype=zip_dtype)\n",
    "geodf_311_data_for_sql.to_sql('complaint', engine, if_exists='append', index=False, dtype=complaint_dtype)\n",
    "geodf_tree_data_for_sql.to_sql('tree', engine, if_exists='append', index=False, dtype=tree_dtype)\n",
    "df_zillow_data.to_sql('zillow', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part3: Understanding Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 write_query_to_file function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function (12) write query to file function\n",
    "def write_query_to_file(query, outfile):\n",
    "    \"\"\"\n",
    "    Writes a SQL query to a specified file.\n",
    "\n",
    "    Args:\n",
    "    - query (str): The SQL query to be written.\n",
    "    - outfile (str): The name of the file to write the query to.\n",
    "\n",
    "    This function creates a file in the 'sql_queries' directory and writes the given SQL query to it.\n",
    "    \"\"\"\n",
    "    # Define the directory path for storing SQL queries\n",
    "    QUERY_DIR = Path('sql_queries')\n",
    "    # Ensure the directory exists\n",
    "    QUERY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    # Construct the complete file path for the output file\n",
    "    filepath = QUERY_DIR / outfile\n",
    "    # Open the file in write mode and write the query\n",
    "    with open(filepath, 'w') as file:\n",
    "        file.write(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
