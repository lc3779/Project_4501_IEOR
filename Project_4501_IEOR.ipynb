{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part0 Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import psycopg2\n",
    "import sqlalchemy as db\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "import contextily as ctx\n",
    "from geoalchemy2 import Geometry\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import unittest\n",
    "import numpy as np\n",
    "from unittest.mock import patch\n",
    "# warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Downloading 311 and Tree data from API\n",
    "- Download 311 and Tree data using API and Python Code\n",
    "- 311 Data set is pretty huge, so we choose to download it seperately into subfiles by year, and merge it at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Function (1) Manually Doanload Data from NYC Open Data\n",
    "def download_data(url, app_token, filename, date_field, start_date, end_date, date_format=\"%Y-%m-%dT%H:%M:%S\", limit=10000):\n",
    "    \"\"\"\n",
    "    Downloads data from the specified NYC Open Data URL within a given date range.\n",
    "\n",
    "    Args:\n",
    "        url (str): The API endpoint for the dataset.\n",
    "        app_token (str): Application token for authenticated access.\n",
    "        filename (str): The name of the file where the data will be saved.\n",
    "        date_field (str): The name of the date field in the dataset.\n",
    "        start_date (datetime): The start date for filtering data.\n",
    "        end_date (datetime): The end date for filtering data.\n",
    "        date_format (str): Format of the date fields, defaults to '%Y-%m-%dT%H:%M:%S'.\n",
    "        limit (int): Number of records to retrieve per request, defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "        None: This function writes the downloaded data to a file and does not return anything.\n",
    "    \"\"\"\n",
    "    \n",
    "    offset = 0\n",
    "    start_date_str = start_date.strftime(date_format) # Format the start date\n",
    "    end_date_str = end_date.strftime(date_format) # Format the end date\n",
    "    # Construct the query for filtering data by date range\n",
    "    date_query = f\"$where={date_field} between '{start_date_str}' and '{end_date_str}'\"\n",
    "    \n",
    "    first_batch = True  # Flag to identify the first batch of data\n",
    "    while True:\n",
    "        # Construct the full URL with necessary query parameters\n",
    "        full_url = f\"{url}?$$app_token={app_token}&{date_query}&$limit={limit}&$offset={offset}\"\n",
    "        response = requests.get(full_url) # Perform the API request\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.text\n",
    "            records_retrieved = data.count('\\n')  # Count the number of lines (records) retrieved\n",
    "\n",
    "            if first_batch and records_retrieved > 0:  # If this is the first batch and it contains data\n",
    "                with open(filename, 'w') as file:\n",
    "                    file.write(data) # Write data to file, including header\n",
    "                first_batch = False\n",
    "            elif records_retrieved > 1:  # For subsequent batches, skip the header row\n",
    "                with open(filename, 'a') as file:\n",
    "                    file.write(data.split('\\n', 1)[1])  # Append data to file without header\n",
    "\n",
    "            if records_retrieved < limit + 1:   # Check if all records have been retrieved\n",
    "                break\n",
    "            offset += limit # Increment the offset for the next batch\n",
    "        else:\n",
    "            print(f\"Failed to download data at offset {offset}: Status code {response.status_code}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#app Toekn: Application token used for authentication\n",
    "app_token = 'Z8lDMDpdnonlT1RjM5YGII6Ii'\n",
    "#Data URL: Defines the online API URLs for the datasets\n",
    "url_311 = 'https://data.cityofnewyork.us/resource/erm2-nwe9.csv'\n",
    "url_trees = 'https://data.cityofnewyork.us/resource/5rq2-4hqu.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
