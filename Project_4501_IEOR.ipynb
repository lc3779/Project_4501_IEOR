{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part0 Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import psycopg2\n",
    "import sqlalchemy as db\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "import contextily as ctx\n",
    "from geoalchemy2 import Geometry\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import unittest\n",
    "import numpy as np\n",
    "from unittest.mock import patch\n",
    "# warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Downloading 311 and Tree data from API\n",
    "- Download 311 and Tree data using API and Python Code\n",
    "- 311 Data set is pretty huge, so we choose to download it seperately into subfiles by year, and merge it at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Function (1) Manually Doanload Data from NYC Open Data\n",
    "def download_data(url, app_token, filename, date_field, start_date, end_date, date_format=\"%Y-%m-%dT%H:%M:%S\", limit=10000):\n",
    "    \"\"\"\n",
    "    Downloads data from the specified NYC Open Data URL within a given date range.\n",
    "\n",
    "    Args:\n",
    "        url (str): The API endpoint for the dataset.\n",
    "        app_token (str): Application token for authenticated access.\n",
    "        filename (str): The name of the file where the data will be saved.\n",
    "        date_field (str): The name of the date field in the dataset.\n",
    "        start_date (datetime): The start date for filtering data.\n",
    "        end_date (datetime): The end date for filtering data.\n",
    "        date_format (str): Format of the date fields, defaults to '%Y-%m-%dT%H:%M:%S'.\n",
    "        limit (int): Number of records to retrieve per request, defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "        None: This function writes the downloaded data to a file and does not return anything.\n",
    "    \"\"\"\n",
    "    \n",
    "    offset = 0\n",
    "    start_date_str = start_date.strftime(date_format) # Format the start date\n",
    "    end_date_str = end_date.strftime(date_format) # Format the end date\n",
    "    # Construct the query for filtering data by date range\n",
    "    date_query = f\"$where={date_field} between '{start_date_str}' and '{end_date_str}'\"\n",
    "    \n",
    "    first_batch = True  # Flag to identify the first batch of data\n",
    "    while True:\n",
    "        # Construct the full URL with necessary query parameters\n",
    "        full_url = f\"{url}?$$app_token={app_token}&{date_query}&$limit={limit}&$offset={offset}\"\n",
    "        response = requests.get(full_url) # Perform the API request\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.text\n",
    "            records_retrieved = data.count('\\n')  # Count the number of lines (records) retrieved\n",
    "\n",
    "            if first_batch and records_retrieved > 0:  # If this is the first batch and it contains data\n",
    "                with open(filename, 'w') as file:\n",
    "                    file.write(data) # Write data to file, including header\n",
    "                first_batch = False\n",
    "            elif records_retrieved > 1:  # For subsequent batches, skip the header row\n",
    "                with open(filename, 'a') as file:\n",
    "                    file.write(data.split('\\n', 1)[1])  # Append data to file without header\n",
    "\n",
    "            if records_retrieved < limit + 1:   # Check if all records have been retrieved\n",
    "                break\n",
    "            offset += limit # Increment the offset for the next batch\n",
    "        else:\n",
    "            print(f\"Failed to download data at offset {offset}: Status code {response.status_code}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#app Toekn: Application token used for authentication\n",
    "app_token = 'Z8lDMDpdnonlT1RjM5YGII6Ii'\n",
    "#Data URL: Defines the online API URLs for the datasets\n",
    "url_311 = 'https://data.cityofnewyork.us/resource/erm2-nwe9.csv'\n",
    "url_trees = 'https://data.cityofnewyork.us/resource/5rq2-4hqu.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download tree data: Initiates the download of tree data\n",
    "# date format \"%m/%d/%Y\": Setting the date format to month/day/year\n",
    "download_data(\n",
    "    url=url_trees,\n",
    "    app_token=app_token,  \n",
    "    filename=\"data/tree_data.csv\",\n",
    "    date_field=\"created_at\",  \n",
    "    start_date=datetime(2015, 1, 1),\n",
    "    end_date=datetime(2015, 12, 31),\n",
    "    date_format=\"%m/%d/%Y\",  \n",
    "    limit=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download 311 data from 2015.1.1-2023.9.30\n",
    "#create a new folder to save 311 data by year\n",
    "subfolder_name = \"311_data\"\n",
    "subfolder_path = os.path.join(\"data\", subfolder_name)\n",
    "if not os.path.exists(subfolder_path):\n",
    "    os.makedirs(subfolder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run download data function by year\n",
    "#Year 2015\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2015.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2015, 1, 1, 0, 0),  # This represents 2015-01-01 00:00:00 AM\n",
    "    end_date=datetime(2015, 12, 31, 23, 59, 59),  # This represents 2023-09-30 11:59:59 PM\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Test (1)\n",
    "# Path to the file you expect to exist\n",
    "filename = \"data/311_data/311_data_2015.csv\"\n",
    "# Assert that the file exists\n",
    "assert os.path.exists(filename), \"File does not exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2016\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2016.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2016, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2016, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2017\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2017.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2017, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2017, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2018\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2018.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2018, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2018, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2019\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2019.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2019, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2019, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2020\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2020.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2020, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2020, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2021\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2021.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2021, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2021, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2022\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2022.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2022, 1, 1, 0, 0), \n",
    "    end_date=datetime(2022, 12, 31, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2023\n",
    "download_data(\n",
    "    url_311,\n",
    "    app_token,  \n",
    "    filename=\"data/311_data/311_data_2023.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2023, 1, 1, 0, 0),  \n",
    "    end_date=datetime(2023, 9, 30, 23, 59, 59),  \n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cleaning & Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Data Description & Columns to keep\n",
    "\n",
    "Prior to developing data cleaning functions, we conducted a thorough examination of each dataset's types and descriptions, as well as the query and visualization requirements of our project. The following list details the columns we have chosen to retain. Each column is accompanied by its description and original data type.\n",
    "\n",
    "##### *Zillow Rent Data Description\n",
    "\n",
    "| Column Name                  | Description                                                                          | Type        |\n",
    "|------------------------------|--------------------------------------------------------------------------------------|-------------|\n",
    "| RegionID                     | Used for pandas, an identifier for the region                                         | Integer     |\n",
    "| RegionName                   | Same as postcode, matches 'Incident Zip' in other datasets, link with latitude and longitude | Integer     |\n",
    "| City                         | Different cities, for later filtering to New York                                     | Object      |\n",
    "| Average Housing Price Columns| Keep all columns related to average housing prices for each region                   | Float64     |\n",
    "\n",
    "\n",
    "##### *311 Data\n",
    "https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9\n",
    "\n",
    "311 Service Requests from 2010 to Present\n",
    "This dataset comprises all 311 Service Requests from 2010 to the present day, updated daily. It provides a comprehensive overview of non-emergency requests and complaints to New York City's 311 service.\n",
    "\n",
    "\n",
    "| Column Name    | Description                                                              | Type        |\n",
    "|----------------|--------------------------------------------------------------------------|-------------|\n",
    "| unique_key     | Unique identifier of a Service Request (SR) in the open data set         | Plain Text  |\n",
    "| created_date   | Date SR was created                                                      | Date & Time |\n",
    "| complaint_type | First level of a hierarchy identifying the topic of the incident or condition | Plain Text  |\n",
    "| incident_zip   | Incident location zip code, provided by geo validation                   | Plain Text  |\n",
    "| latitude       | Geo based Latitude of the incident location                              | Number      |\n",
    "| longitude      | Geo based Longitude of the incident location                             | Number      |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### *Tree Data Description\n",
    "https://data.cityofnewyork.us/Environment/2015-Street-Tree-Census-Tree-Data/uvpi-gqnh\n",
    "\n",
    "\n",
    "2015 Street Tree Census, conducted by volunteers and staff organized by NYC Parks & Recreation and partner organizations. Tree data collected includes tree species, diameter and perception of health. Accompanying blockface data is available indicating status of data collection and data release citywide.\n",
    "\n",
    "\n",
    "| Column Name   | Description                                                                   | Type       |\n",
    "|---------------|-------------------------------------------------------------------------------|------------|\n",
    "| tree_id       | Unique identification number for each tree point                              | Integer     |\n",
    "| status        | Indicates whether the tree is alive, standing dead, or a stump               | Plain Text |\n",
    "| created_at  | Date and time when the tree data was created                                  | Plain Text |\n",
    "| zipcode       | Five-digit zipcode in which tree is located                                  | Integer |\n",
    "| latitude      | Latitude of point, in decimal degrees                                        | Number     |\n",
    "| longitude     | Longitude of point, in decimal degrees                                       | Number     |\n",
    "| health        | Indicates the user's perception of tree health                               | Plain Text |\n",
    "| spc_common    | Common name for species, e.g., \"red maple\"                                   | Plain Text |\n",
    "\n",
    "\n",
    "#### *Zipcode Data Description\n",
    "| Column Name | Description                                | Type    |\n",
    "|-------------|--------------------------------------------|---------|\n",
    "| ZIPCODE     | The postal code corresponding to the area  | Plain Text |\n",
    "| geometry    | Geometrical data representing the area     | Geometry |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2.1 Functions aiding data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function (2) create a remove_column function for deleting the unnecessary columns\n",
    "\n",
    "def remove_column(df, keep_columns, include_date_columns=False, date_pattern=r'\\d{4}-\\d{2}-\\d{2}'):\n",
    "    \"\"\"\n",
    "    Removes columns from a DataFrame, retaining only the specified columns and optionally any date columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to be modified.\n",
    "    - keep_columns (list of str): A list of column names to retain in the DataFrame.\n",
    "    - include_date_columns (bool, optional): Flag to include columns with date format. Defaults to False.\n",
    "    - date_pattern (str, optional): Regular expression pattern to identify date columns. Used when 'include_date_columns' is True. Default pattern matches 'YYYY-MM-DD'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The modified DataFrame with only the specified columns retained.\n",
    "    \n",
    "    Example:\n",
    "    >>> df = pd.DataFrame(...)\n",
    "    >>> new_df = remove_column(df, ['column1', 'column2'], include_date_columns=True)\n",
    "    \"\"\"\n",
    "    # Combine specified columns with date columns if needed\n",
    "    all_columns_to_keep = keep_columns\n",
    "\n",
    "    # If including date columns, append them to the list of columns to keep\n",
    "    if include_date_columns:\n",
    "        # Identify date columns using the regex pattern\n",
    "        date_columns = df.columns[df.columns.str.contains(date_pattern)]\n",
    "        all_columns_to_keep += date_columns.tolist()\n",
    "\n",
    "    # Return the DataFrame with only the specified columns retained\n",
    "    return df.loc[:, all_columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test (2)\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'column1': [1, 2, 3],\n",
    "    'column2': [4, 5, 6],\n",
    "    'date_column': pd.to_datetime(['2020-01-01', '2020-01-02', '2020-01-03']),\n",
    "    'irrelevant_column': ['x', 'y', 'z']\n",
    "})\n",
    "\n",
    "# Run the function with a test case\n",
    "keep_columns = ['column1', 'column2']\n",
    "new_df = remove_column(df, keep_columns, include_date_columns=True)\n",
    "\n",
    "# Test assertions\n",
    "assert 'column1' in new_df.columns, \"column1 is not in the DataFrame\"\n",
    "assert 'column2' in new_df.columns, \"column2 is not in the DataFrame\"\n",
    "assert 'date_column' not in new_df.columns, \"date_column should not be retained in the DataFrame based on the current function implementation\"\n",
    "assert 'irrelevant_column' not in new_df.columns, \"irrelevant_column should have been removed from the DataFrame\"\n",
    "assert new_df.shape[1] == 2, \"The number of columns in the DataFrame should be two\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function (3),(4) create functions to check valid longitute and latitute\n",
    "def is_valid_latitude(lat):\n",
    "    \"\"\"\n",
    "    Checks if the provided latitude value is valid.\n",
    "\n",
    "    Parameters:\n",
    "    - lat (str): The latitude value to be checked.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the latitude is valid, False otherwise.\n",
    "\n",
    "    A valid latitude is a number between -90 and 90.\n",
    "    Non-numeric values or latitudes outside this range will return False.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Check if latitude is within the valid range\n",
    "        return -90 <= float(lat) <= 90\n",
    "    except ValueError:\n",
    "        # Return False if latitude is not a number\n",
    "        return False\n",
    "\n",
    "def is_valid_longitude(lon):\n",
    "    \"\"\"\n",
    "    Checks if the provided longitude value is valid.\n",
    "\n",
    "    Parameters:\n",
    "    - lon (str): The longitude value to be checked.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the longitude is valid, False otherwise.\n",
    "\n",
    "    A valid longitude is a number between -180 and 180.\n",
    "    Non-numeric values or longitudes outside this range will return False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if longitude is within the valid range\n",
    "        return -180 <= float(lon) <= 180\n",
    "    except ValueError:\n",
    "        # Return False if longitude is not a number\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Test (3) cases for is_valid_latitude\n",
    "assert is_valid_latitude(\"45\"), \"45 should be a valid latitude\"\n",
    "assert is_valid_latitude(\"-90\"), \"-90 should be a valid latitude\"\n",
    "assert is_valid_latitude(\"90\"), \"90 should be a valid latitude\"\n",
    "assert not is_valid_latitude(\"100\"), \"100 should not be a valid latitude\"\n",
    "assert not is_valid_latitude(\"-91\"), \"-91 should not be a valid latitude\"\n",
    "assert not is_valid_latitude(\"not_a_number\"), \"'not_a_number' should not be a valid latitude\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Test (4) cases for is_valid_longitude\n",
    "assert is_valid_longitude(\"90\"), \"90 should be a valid longitude\"\n",
    "assert is_valid_longitude(\"-180\"), \"-180 should be a valid longitude\"\n",
    "assert is_valid_longitude(\"180\"), \"180 should be a valid longitude\"\n",
    "assert not is_valid_longitude(\"190\"), \"190 should not be a valid longitude\"\n",
    "assert not is_valid_longitude(\"-181\"), \"-181 should not be a valid longitude\"\n",
    "assert not is_valid_longitude(\"not_a_number\"), \"'not_a_number' should not be a valid longitude\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2.2 Functions performing data cleaning and filtering for each datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. zipcode\n",
    "# Function (5) Create a clean and filter function for zipcode data\n",
    "def process_zipcode_shapefile(shapefile_path):\n",
    "    \"\"\"\n",
    "    Processes and cleans the zipcode shapefile to prepare it for analysis.\n",
    "\n",
    "    Args:\n",
    "    - shapefile_path (str): Path to the zipcode shapefile.\n",
    "\n",
    "    Returns:\n",
    "    - gpd.GeoDataFrame: A GeoDataFrame containing the cleaned and processed zipcode data.\n",
    "\n",
    "    This function performs several steps to clean the data, including selecting essential columns,\n",
    "    removing duplicates, filtering out missing values, renaming and reformatting columns, \n",
    "    and adjusting geometries to a common Coordinate Reference System (CRS).\n",
    "    \"\"\"\n",
    "    # Step 1: Reading the shapefile\n",
    "    zipcode_gdf = gpd.read_file(shapefile_path)\n",
    "    \n",
    "    # Step 2: Selecting essential columns\n",
    "    columns_required = ['ZIPCODE', 'geometry']\n",
    "    zipcode_gdf = zipcode_gdf[columns_required]\n",
    "    \n",
    "    \n",
    "    # Step 3: Remove duplicates\n",
    "    zipcode_gdf.drop_duplicates(subset=['ZIPCODE'], inplace=True)\n",
    "    \n",
    "    \n",
    "    # Step 4: Filtering out rows with missing values in critical columns\n",
    "    zipcode_gdf.dropna(subset=['ZIPCODE', 'geometry'], inplace=True)\n",
    "    \n",
    "    \n",
    "    # Step 5: Renaming and reformatting the columns\n",
    "    # Renaming 'ZIPCODE' to 'zipcode' for consistency\n",
    "    zipcode_gdf.rename(columns={'ZIPCODE': 'zipcode'}, inplace=True)\n",
    "    \n",
    "        \n",
    "    # Converting the 'zipcode' to string format for standardization\n",
    "    zipcode_gdf['zipcode'] = zipcode_gdf['zipcode'].astype(int)  # Convert zipcode to integer\n",
    "    \n",
    "    \n",
    "    # check if it lasts with 5 digits\n",
    "    zipcode_gdf = zipcode_gdf[zipcode_gdf['zipcode'].apply(lambda x: str(x).isdigit() and len(str(x)) == 5)]\n",
    "\n",
    "    # Step 6: Adjusting geometries to a common CRS\n",
    "    # Transforming geometries to the web mercator projection for mapping compatibility\n",
    "    common_crs = \"EPSG:3857\"\n",
    "    zipcode_gdf.to_crs(common_crs, inplace=True)\n",
    "    \n",
    "    # Ensuring all column names are in lowercase\n",
    "    zipcode_gdf.columns = map(str.lower, zipcode_gdf.columns)\n",
    "    \n",
    "    return zipcode_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 311 Data\n",
    "# Function (6)\n",
    "def clean_filter_311(filename, nyc_zipcodes):\n",
    "    \"\"\"\n",
    "    Cleans and filters 311 complaint data to retain only relevant information and valid entries.\n",
    "\n",
    "    Args:\n",
    "    - filename (str): Path to the CSV file containing 311 data.\n",
    "    - nyc_zipcodes (list of int): List of valid NYC zip codes for filtering.\n",
    "\n",
    "    Returns:\n",
    "    - gpd.GeoDataFrame: A GeoDataFrame containing the cleaned and filtered 311 complaint data.\n",
    "\n",
    "    The function includes data loading, column selection, data validation, normalization of column names and types, \n",
    "    and transformation of data into a geospatial format.\n",
    "    \"\"\"\n",
    "    # Step 0: Load data\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Step 1: Select necessary columns\n",
    "    columns_to_keep_311 = ['unique_key', 'created_date', 'complaint_type', 'incident_zip', 'latitude', 'longitude']\n",
    "    df = df[columns_to_keep_311]\n",
    "\n",
    "    # Step 2: Remove invalid and duplicate data\n",
    "    # Remove duplicates and confirm unique_key\n",
    "    df = df.drop_duplicates(subset='unique_key')\n",
    "    df = df[df['unique_key'].notna() & df['unique_key'].apply(lambda x: str(x).isdigit())]\n",
    "\n",
    "    # Remove rows with NaN in critical columns\n",
    "    df = df.dropna(subset=['incident_zip', 'latitude', 'longitude', 'created_date'])\n",
    "\n",
    "    # Check if latitude and longitude are valid\n",
    "    df = df[df['latitude'].apply(is_valid_latitude) & df['longitude'].apply(is_valid_longitude)]\n",
    "\n",
    "    # Step 3: Normalize Column Names, Column Types\n",
    "    \n",
    "    # Standardize 'incident_zip' to integer and filter by NYC zipcodes\n",
    "    df['incident_zip'] = df['incident_zip'].astype(int)\n",
    "    df = df[df['incident_zip'].isin(nyc_zipcodes)]\n",
    "\n",
    "    # Rename and reformat columns for consistency\n",
    "    df.rename(columns={'unique_key': 'complaint_id', 'created_date': 'date', 'incident_zip': 'zipcode'}, inplace=True)\n",
    "    df['complaint_id'] = df['complaint_id'].astype(int)\n",
    "\n",
    "    # Convert 'date' column to datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Drop rows with null values post transformations\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Step 4: Normalize the SRID of any geometry\n",
    "    \n",
    "    # Convert DataFrame to GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['longitude'], df['latitude']))\n",
    "    gdf.set_crs(\"EPSG:4326\", inplace=True)\n",
    "    gdf.to_crs(\"EPSG:3857\", inplace=True)\n",
    "\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Function (7) to clean and merge 311 data\n",
    "def clean_merge_311_data(folder_path, file_prefix, years,nyc_zipcodes):\n",
    "    \"\"\"\n",
    "    Cleans individual 311 data files for specified years and merges them into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - folder_path (str): Path to the folder containing 311 data files.\n",
    "    - file_prefix (str): Prefix of the 311 data filenames.\n",
    "    - years (list of int): List of years for which the 311 data needs to be cleaned and merged.\n",
    "    - nyc_zipcodes (list of int): List of valid NYC zip codes for filtering.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A merged DataFrame containing cleaned and consolidated 311 data for the specified years.\n",
    "\n",
    "    The function iteratively cleans each year's data file using 'clean_filter_311' and then merges them into one DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # List to hold cleaned DataFrames for each year\n",
    "    cleaned_dfs = []\n",
    "    \n",
    "    # Iterate over each year, clean the corresponding 311 data file, and add it to the list\n",
    "    for year in years:\n",
    "        filename = f\"{folder_path}/{file_prefix}{year}.csv\"\n",
    "        cleaned_df = clean_filter_311(filename,nyc_zipcodes)\n",
    "        cleaned_dfs.append(cleaned_df)\n",
    "    \n",
    "    # # Merge all cleaned DataFrames into a single DataFrame\n",
    "    merged_df = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Function (8) to clean and filter tree data\n",
    "def clean_filter_tree(filename, nyc_zipcodes):\n",
    "    \"\"\"\n",
    "    Cleans and filters tree data, retaining only the relevant information and valid entries.\n",
    "\n",
    "    Args:\n",
    "    - filename (str): Path to the CSV file containing tree data.\n",
    "    - nyc_zipcodes (list of int): List of valid NYC zip codes for filtering.\n",
    "\n",
    "    Returns:\n",
    "    - gpd.GeoDataFrame: A GeoDataFrame containing the cleaned and filtered tree data.\n",
    "\n",
    "    This function includes loading data, selecting necessary columns, removing invalid and duplicate data,\n",
    "    and transforming the data into a geospatial format.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Select the necessary columns for analysis\n",
    "    columns_to_keep_tree = ['tree_id', 'status', 'zipcode', 'latitude', 'longitude', 'health', 'spc_common', 'created_at']\n",
    "    df = df[columns_to_keep_tree]\n",
    "\n",
    "    # Remove duplicate entries and ensure tree IDs are valid (tree id as uid)\n",
    "    df = df.drop_duplicates(subset='tree_id')\n",
    "    df = df[df['tree_id'].notna() & df['tree_id'].apply(lambda x: str(x).isdigit())]\n",
    "\n",
    "    # Drop rows with missing values in critical columns before type conversions\n",
    "    df = df.dropna(subset=['zipcode', 'latitude', 'longitude', 'created_at'])\n",
    "\n",
    "    # Standardize 'zipcode' to integer and filter by NYC zipcodes\n",
    "    df['zipcode'] = df['zipcode'].astype(int)\n",
    "    df = df[df['zipcode'].isin(nyc_zipcodes)]\n",
    "\n",
    "    # Validate latitude and longitude values\n",
    "    df = df[df['latitude'].apply(is_valid_latitude) & df['longitude'].apply(is_valid_longitude)]\n",
    "\n",
    "    # Rename and reformat columns for consistency\n",
    "    df.rename(columns={'created_at': 'date', 'spc_common': 'species'}, inplace=True)\n",
    "\n",
    "    # Convert 'date' column to datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Convert to GeoDataFrame and normalize SRID\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['longitude'], df['latitude']))\n",
    "    gdf.set_crs(\"EPSG:4326\", inplace=True)\n",
    "    gdf.to_crs(\"EPSG:3857\", inplace=True)\n",
    "\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Function (9) to clean and filter zillow rent data\n",
    "def clean_filter_rent(filename, nyc_zipcodes):\n",
    "    \"\"\"\n",
    "    Cleans and filters Zillow rent data, retaining only the relevant information for NYC regions.\n",
    "\n",
    "    Args:\n",
    "    - filename (str): Path to the CSV file containing Zillow rent data.\n",
    "    - nyc_zipcodes (list of int): List of valid NYC zip codes for filtering.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing the cleaned and filtered Zillow rent data.\n",
    "\n",
    "    This function includes loading the data, dropping unnecessary columns, renaming columns, \n",
    "    and transforming the data format for analysis.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Drop 'City' column\n",
    "    df = df.drop(columns=['City'])\n",
    "    \n",
    "    # Rename 'RegionID' and 'RegionName' columns for standardization\n",
    "    df.rename(columns={'RegionID': 'region_id', 'RegionName': 'zipcode'}, inplace=True)\n",
    "    \n",
    "    # Remove duplicate entries based on 'region_id'\n",
    "    df = df.drop_duplicates(subset='region_id')\n",
    "    \n",
    "    # Drop rows with NA values in 'zipcode' column\n",
    "    df = df.dropna(subset=['zipcode'])\n",
    "\n",
    "    # Ensure that the 'zipcode' is an integer format for consistency\n",
    "    df['zipcode'] = df['zipcode'].astype(int)\n",
    "\n",
    "    # Filter the DataFrame to include only rows with NYC zip codes\n",
    "    df = df[df['zipcode'].isin(nyc_zipcodes)]\n",
    "    \n",
    "    # Transpose the dataframe: each date becomes a row\n",
    "    id_vars = ['region_id', 'zipcode']\n",
    "    value_vars = [col for col in df.columns if col not in id_vars and col.startswith('20')]\n",
    "    df_melted = df.melt(id_vars=id_vars, value_vars=value_vars, var_name='date', value_name='rent')\n",
    "    \n",
    "    # Add 'rent_id' column as an identifier to uniquely identify each rent entry\n",
    "    df_melted.insert(0, 'rent_id', range(1, 1 + len(df_melted)))\n",
    "    \n",
    "    # Filter out rows with NaN values in any column\n",
    "    df_melted = df_melted.dropna()\n",
    "    \n",
    "    return df_melted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#311 data input arguments\n",
    "folder_path = \"data/311_data\"\n",
    "file_prefix = \"311_data_\"\n",
    "years = ['2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
    "\n",
    "# Function (10) load all data\n",
    "def load_all_data():\n",
    "    \"\"\"\n",
    "    Loads and processes all datasets: Zipcode, 311, Tree, and Zillow Rent data.\n",
    "\n",
    "    This function first processes the NYC zipcode data to create a list of valid NYC zipcodes. \n",
    "    It then uses this list to filter and process the 311, Tree, and Zillow Rent datasets.\n",
    "\n",
    "    Returns:\n",
    "    Tuple: Containing GeoDataFrames for Zipcode and 311 data, and DataFrames for Tree and Zillow Rent data.\n",
    "    \"\"\"\n",
    "    # Process the zipcode shapefile to create a GeoDataFrame of NYC zipcodes\n",
    "    geodf_zipcode_data = process_zipcode_shapefile(\"data/nyc_zipcodes/ZIP_CODE_040114.shp\")\n",
    "    \n",
    "    # Extract a list of valid NYC zipcodes from the processed zipcode data\n",
    "    nyc_zipcodes = geodf_zipcode_data['zipcode'].tolist()\n",
    "    \n",
    "     # Clean and merge 311 data for the specified years using the list of NYC zipcodes\n",
    "    geodf_311_data = clean_merge_311_data(folder_path, file_prefix, years,nyc_zipcodes)\n",
    "    \n",
    "    # Clean and filter tree data using the list of NYC zipcodes\n",
    "    geodf_tree_data = clean_filter_tree('data/tree_data.csv',nyc_zipcodes)\n",
    "    \n",
    "    # Clean and filter Zillow Rent data using the list of NYC zipcodes\n",
    "    df_zillow_data = clean_filter_rent('data/zillow_rent_data.csv',nyc_zipcodes)\n",
    "    \n",
    "    # Return all processed datasets\n",
    "    return (\n",
    "        geodf_zipcode_data,\n",
    "        geodf_311_data,\n",
    "        geodf_tree_data,\n",
    "        df_zillow_data\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
